You are an expert full‐stack AI/Next.js developer. Your task is to generate an entire Next.js codebase (written in TypeScript) for a project named **DreamScape**. DreamScape is a web application that:

1. Accepts user input in the form of a written dream journal entry (200–500 words).
2. Uses large‐language AI (e.g., OpenAI’s GPT-4o) to parse that journal entry:  
   a. Extracts key “location” descriptions (e.g., “glass city,” “mysterious forest”),  
   b. Identifies primary objects or characters (e.g., “floating orbs,” “ancient library”),  
   c. Detects mood keywords or atmosphere indicators (e.g., “surreal,” “ominous,” “serene”),  
   d. Outputs a structured JSON that contains a list of “Scenes,” each with:  
      • `sceneTitle`: a brief name (“Glass City Plaza”).  
      • `sceneDescription`: a 2–3 sentence summary of that scene (extracted and rewritten from the dream text).  
      • `objects`: an array of strings (each string is one object or character in that scene).  
      • `locationKeywords`: an array of strings capturing physical or environmental cues.  
      • `moodKeywords`: an array of strings describing the emotional tone.  
      • `promptKeywords`: a concatenation of location + object + mood phrases (e.g., “glass city, floating orbs, serene, blue light”) to feed into an AI‐image generator.

3. Takes each `promptKeywords` per scene and requests an AI‐image-generation API (e.g., Stable Diffusion) to produce a **360° or wide‐angle background texture** (minimum resolution 4096×1024) suitable for a WebGL “skybox” or equirectangular panorama.

4. Constructs an interactive 3D scene in the browser for each scene using **React Three Fiber** (the React‐friendly wrapper around Three.js). Specifically:  
   a. Creates a “Sky” (panorama) mesh or `<Skybox>` component that shows the AI‐generated 360° image as the environment.  
   b. For each `objects` entry, generates or imports a low-poly 3D model (if available from an open source library) or uses simple placeholder geometry (e.g., spheres, cubes) with text overlays. Each object should be placed in the 3D world according to a random but coherent arrangement (no overlapping).  
   c. Applies a color filter or post‐processing effect based on `moodKeywords` (if “serene,” apply a mild bloom; if “ominous,” apply slight fog with a dark tint).  
   d. Provides standard controls (WASD keys or arrow keys + mouse/touch) to “walk around” or “fly” through the 3D scene.

5. Generates a **short ambient audio loop** for each scene using an AI audio‐generation API (e.g., an ElevenLabs TTS or background sound generation model). The ambient audio must reflect the `moodKeywords` (e.g., a calm, ethereal pad for “serene,” a subtle low drone for “ominous”). The audio files should be fetched as `.mp3` or `.ogg` URLs.

6. Displays a “Scene Viewer” UI:  
   - A page or panel where the user can switch between different scenes (e.g., a vertical list of scene titles on the left, or a “Next Scene” / “Previous Scene” button).  
   - When the user selects a scene:  
     • The 3D canvas updates to show that scene’s panorama + objects + audio.  
     • A subtitles/tooltip box shows the `sceneDescription` in plain text.  
     • An “Audio On/Off” toggle controls playback of the ambient loop.  
     • A “Download” button lets the user download the current scene’s 360° image (as a `.jpg`) and audio file (as `.mp3`) in a ZIP or individually.

7. **Summary of User Flows**  
 - The user lands on the homepage (`/`).  
 - They see a textarea labeled “Type or paste your dream…” and a button “Analyze Dream →.”  
 - They enter at least 50 characters, click “Analyze Dream →.”  
   • A loading spinner overlays the screen.  
   • Next.js calls `/api/analyze-dream`. GPT returns `scenes: SceneData[]`.  
   • For each `SceneData`, the front end simultaneously calls `/api/generate-image` and `/api/generate-audio`.  
   • When all imageUrl/audioUrl pairs arrive, the scenes populate the sidebar.  
 - The user clicks a scene title in the sidebar.  
   • The `<SceneViewer>` component renders a `<Canvas>` Three.js scene with:  
     – A spherical skybox textured with `imageUrl`.  
     – Placeholder objects arranged around the user, each labeled with the object string.  
     – Lighting + postprocessing based on `moodKeywords`.  
   • The overlay shows `sceneDescription`.  
   • The user can click “Play Audio” to start the ambient loop, or “Pause Audio.”  
   • The user can click “Download Image” to save the panorama or “Download Audio” to save the MP3.  
 - The user can click “Analyze Dream →” again with new text to get a fresh set of scenes.

8. **Edge Cases & Error Handling**  
 - If `/api/analyze-dream` returns fewer than 1 scene in the `scenes` array, show an error “Sorry, we couldn’t find distinct scenes in your dream—try rephrasing.”  
 - If any single `/api/generate-image` or `/api/generate-audio` call fails, skip that scene’s image or audio but still display the scene. If audio fails, show a single “Audio generation failed” toast and disable the Play button. If image fails, substitute a local fallback image (`/public/fallback-panorama.jpg`) so the user still sees a skybox.  
 - Validate user input length: enforce minimum 50 characters, maximum 2000 characters. If out of range, disable the “Analyze Dream →” button and show a tooltip: “Dream must be between 50 and 2000 characters.”  
 - Provide a “Try Again” button if global errors occur (e.g., GPT times out).